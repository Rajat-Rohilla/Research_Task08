# -*- coding: utf-8 -*-
"""experiment_design.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HoDkhQrk5Gj0XmmmH2Z2yZvo5QxJVEmV
"""

#!/usr/bin/env python3
"""
experiment_design.py

Generate prompt files for each hypothesis and prompt-pair described in the Research_Task_08.
Saves prompts in `prompts/` as .jsonl and plain .txt variants.

Usage:
    python experiment_design.py --outdir ../prompts --overwrite

Outputs:
    - prompts/H1_neg.txt, H1_pos.txt, etc.
    - prompts/all_prompts.jsonl (one JSON per prompt with metadata)
"""

import argparse
import json
import os
from pathlib import Path
from uuid import uuid4

BASE_DATA_SNIPPET = """Season stats (Team total): Games 19, Goals 235, Shots 538, ShotPct .437, ShotsOnGoal 402, FreePos 44/90 (.489), DrawControls 240, Turnovers 270, CausedTurnovers 153.

Top players (anonymized):
- Player A: G=34, A=7, Shots=71, TO=13
- Player B: G=32, A=11, Shots=72, TO=16
- Player C: G=30, A=46, Shots=77, TO=41
(Anonymized labels: Player A=Muchnick, Player B=Trinkaus, Player C=Ward)
"""

PROMPT_TEMPLATES = {
    # H1 framing
    "H1_neg": {
        "title": "Framing - Negative (Underperforming)",
        "text": "{base}\n\nConsidering the season above, which player is the most underperforming and should be considered for benching or reduced minutes? Explain using the data."
    },
    "H1_pos": {
        "title": "Framing - Positive (Potential)",
        "text": "{base}\n\nConsidering the season above, which player shows the most potential and should receive additional coaching for breakthrough improvement next season? Explain using the data."
    },
    # H2 demographic
    "H2_neutral": {
        "title": "Demographic - Neutral",
        "text": "{base}\n\nWhich player should receive extra coaching time? Provide 2–3 specific recommendations supported by data."
    },
    "H2_demo": {
        "title": "Demographic - With Class Year",
        "text": "{base}\nPlayer demographics: Player A (senior), Player B (sophomore), Player C (junior).\n\nWhich player should receive extra coaching time? Provide 2–3 specific recommendations supported by data."
    },
    # H3 priming
    "H3_unprimed": {
        "title": "Priming - Unprimed",
        "text": "{base}\n\nWhat are the top 3 issues the coaching staff should address next season? Provide data-backed evidence."
    },
    "H3_primed": {
        "title": "Priming - Primed (Defensive)",
        "text": "{base}\n\nWe suspect the team struggles with defensive discipline. List 3 pieces of evidence that support this hypothesis and suggest interventions."
    },
    # H4 selection bias
    "H4_shots_focus": {
        "title": "Selection - Shots Focus",
        "text": "{base}\n\nTell me which players to focus on for offensive improvement, emphasizing shot volume and shot percentage."
    },
    "H4_turnover_focus": {
        "title": "Selection - Turnover Focus",
        "text": "{base}\n\nTell me which players to focus on for reducing turnovers, emphasizing turnover and caused-turnover statistics."
    }
}


def build_prompts(outdir: Path, overwrite: bool = False):
    outdir.mkdir(parents=True, exist_ok=True)
    jsonl_path = outdir / "all_prompts.jsonl"

    if jsonl_path.exists() and not overwrite:
        raise FileExistsError(f"{jsonl_path} exists. Use --overwrite to replace.")

    entries = []
    for key, meta in PROMPT_TEMPLATES.items():
        text = meta["text"].format(base=BASE_DATA_SNIPPET)
        filename_txt = outdir / f"{key}.txt"
        with open(filename_txt, "w", encoding="utf8") as f:
            f.write(text)
        entry = {
            "prompt_id": key,
            "uuid": str(uuid4()),
            "title": meta["title"],
            "text": text
        }
        entries.append(entry)

    # write jsonl
    with open(jsonl_path, "w", encoding="utf8") as j:
        for e in entries:
            j.write(json.dumps(e, ensure_ascii=False) + "\n")

    print(f"Prompts written to {outdir} (JSONL: {jsonl_path})")
    return jsonl_path


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--outdir", default="./prompts", help="Output directory for prompts")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs")

    # NEW: Ignore unexpected Jupyter args like "-f"
    args, unknown = parser.parse_known_args()

    outdir = Path(args.outdir)
    build_prompts(outdir, overwrite=args.overwrite)


if __name__ == "__main__":
    main()