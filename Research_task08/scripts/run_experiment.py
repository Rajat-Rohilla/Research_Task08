# -*- coding: utf-8 -*-
"""run_experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_aMqntl1ZDAwaHr1dT2g0dZX1Mm86f7t
"""

#!/usr/bin/env python3
"""
run_experiment.py

Command-line utility to run prompts against LLM backends and save responses in NDJSON.

Design choices:
- Pluggable backend: define a client wrapper function `call_model(prompt, model_config)` that you should
  implement for your environment (OpenAI, Anthropic, Google, local LLM).
- Default behavior: the script will read `prompts/*.jsonl` (or .txt), run the model, and save results as NDJSON.

Usage:
    python run_experiment.py --prompts ../prompts/all_prompts.jsonl --models openai:gpt-4 --replicates 3 --out results/h1_runs.ndjson

NOTES:
- This script does not include API keys. Add them to env vars or update wrappers.
- See TODOs in the `call_model` function for adding your chosen provider's code.
"""

import argparse
import json
import os
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
import openai

import os
os.environ["OPENAI_API_KEY"] = "OPENAI_API_KEY"

# ---- Configuration defaults ----
DEFAULT_TEMPERATURE = 0.0
DEFAULT_MAX_TOKENS = 512

# ---- Utility / placeholder for model calls ----


def call_model_openai(prompt: str, model: str = "gpt-4", temperature: float = 0.0, max_tokens: int = 512) -> dict:
    """
    Updated OpenAI API call for openai>=1.0.0
    """
    client = openai.OpenAI()  # initialize client using env API key

    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=max_tokens
    )

    text = resp.choices[0].message.content
    tokens_used = getattr(resp.usage, "total_tokens", None)
    
    return {
        "text": text,
        "tokens": tokens_used,
        "model": model
    }


def call_model_generic(prompt: str, provider: str, model: str, temperature: float, max_tokens: int) -> Dict[str, Any]:
    """
    Dispatch to the provider-specific wrapper. Extend this for other providers.
    """
    if provider.lower() in ("openai", "gpt"):
        return call_model_openai(prompt, model=model, temperature=temperature, max_tokens=max_tokens)
    # Add other providers as needed (anthropic, google, local)
    # e.g., if provider == 'anthropic': return call_anthropic(prompt, model, ...)
    # For now fallback to OpenAI placeholder
    return call_model_openai(prompt, model=model, temperature=temperature, max_tokens=max_tokens)


def read_prompts(prompts_path: Path) -> List[Dict[str, Any]]:
    prompts = []
    if prompts_path.suffix == ".jsonl":
        with open(prompts_path, "r", encoding="utf8") as f:
            for line in f:
                prompts.append(json.loads(line))
    elif prompts_path.is_dir():
        # read all .txt files in dir
        for p in sorted(prompts_path.glob("*.txt")):
            with open(p, "r", encoding="utf8") as f:
                prompts.append({"prompt_id": p.stem, "title": p.stem, "text": f.read()})
    else:
        # single txt
        with open(prompts_path, "r", encoding="utf8") as f:
            prompts.append({"prompt_id": prompts_path.stem, "title": prompts_path.stem, "text": f.read()})
    return prompts


def run_batch(prompts: List[Dict[str, Any]], model_specs: List[str], replicates: int, temperature: float, out_path: Path):
    """
    model_specs: list like ["openai:gpt-4", "openai:gpt-4o-mini"]
    """
    out_path.parent.mkdir(parents=True, exist_ok=True)
    runs = []
    for spec in model_specs:
        provider, model = spec.split(":", 1) if ":" in spec else ("openai", spec)
        for p in prompts:
            for rep in range(replicates):
                run_id = str(uuid.uuid4())
                ts = datetime.utcnow().isoformat() + "Z"
                try:
                    resp = call_model_generic(p["text"], provider=provider, model=model, temperature=temperature, max_tokens=DEFAULT_MAX_TOKENS)
                    record = {
                        "run_id": run_id,
                        "timestamp_utc": ts,
                        "prompt_id": p.get("prompt_id"),
                        "prompt_title": p.get("title"),
                        "prompt_text": p.get("text"),
                        "model_provider": provider,
                        "model": model,
                        "temperature": temperature,
                        "response_text": resp["text"],
                        "response_tokens": resp.get("tokens"),
                        "replicate": rep,
                        "notes": ""
                    }
                except Exception as e:
                    record = {
                        "run_id": run_id,
                        "timestamp_utc": ts,
                        "prompt_id": p.get("prompt_id"),
                        "prompt_title": p.get("title"),
                        "prompt_text": p.get("text"),
                        "model_provider": provider,
                        "model": model,
                        "temperature": temperature,
                        "response_text": None,
                        "response_tokens": None,
                        "replicate": rep,
                        "notes": f"error: {repr(e)}"
                    }
                # append and write line-by-line for streaming safety
                with open(out_path, "a", encoding="utf8") as fh:
                    fh.write(json.dumps(record, ensure_ascii=False) + "\n")
                runs.append(record)
                # small sleep to avoid rate-limit bursts
                time.sleep(0.25)
    return runs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--prompts", required=True, help="Path to prompts JSONL or directory or plain txt")
    parser.add_argument("--models", required=True, help="Comma-separated model specs: provider:model (e.g. openai:gpt-4,openai:gpt-4o-mini)")
    parser.add_argument("--replicates", type=int, default=3, help="Number of replicates per prompt")
    parser.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE)
    parser.add_argument("--out", required=True, help="Output NDJSON file path (append-safe)")
    args = parser.parse_args()

    prompts_path = Path(args.prompts)
    prompts = read_prompts(prompts_path)
    model_list = [m.strip() for m in args.models.split(",") if m.strip()]
    out_path = Path(args.out)

    print(f"Running {len(prompts)} prompts x {len(model_list)} models x {args.replicates} replicates => writing to {out_path}")
    run_batch(prompts, model_list, args.replicates, args.temperature, out_path)
    print("done.")


if __name__ == "__main__":
    main()
